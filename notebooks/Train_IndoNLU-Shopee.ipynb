{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "framed-bahrain",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import sys\n",
    "import inspect\n",
    "import torch\n",
    "from torch import optim\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "from tqdm import tqdm\n",
    "from transformers import BertConfig, BertTokenizer\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "rational-works",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set current notebook path\n",
    "current_dir = os.path.dirname(os.path.abspath(inspect.getfile(inspect.currentframe())))\n",
    "parent_dir = os.path.dirname(current_dir)\n",
    "sys.path.insert(0, parent_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "intellectual-overview",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertConfig, BertTokenizer \n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "from NER_Module.model import BertForWordClassification#, forward_word_classification\n",
    "from NER_Module.utils import ner_metrics_fn, get_lr, metrics_to_string, count_param, set_seed\n",
    "from NER_Module.data import NerGritDataset, NerDataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "posted-antarctica",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Forward function for word classification\n",
    "def forward_word_classification(model, batch_data, i2w, is_test=False, device='cpu', **kwargs):\n",
    "    # Unpack batch data\n",
    "    if len(batch_data) == 4:\n",
    "        (subword_batch, mask_batch, subword_to_word_indices_batch, label_batch) = batch_data\n",
    "        token_type_batch = None\n",
    "    elif len(batch_data) == 5:\n",
    "        (subword_batch, mask_batch, token_type_batch, subword_to_word_indices_batch, label_batch) = batch_data\n",
    "    \n",
    "    # Prepare input & label\n",
    "    subword_batch = torch.LongTensor(subword_batch)\n",
    "    mask_batch = torch.FloatTensor(mask_batch)\n",
    "    token_type_batch = torch.LongTensor(token_type_batch) if token_type_batch is not None else None\n",
    "    subword_to_word_indices_batch = torch.LongTensor(subword_to_word_indices_batch)\n",
    "    label_batch = torch.LongTensor(label_batch)\n",
    "\n",
    "    if device == \"cuda\":\n",
    "        subword_batch = subword_batch.cuda()\n",
    "        mask_batch = mask_batch.cuda()\n",
    "        token_type_batch = token_type_batch.cuda() if token_type_batch is not None else None\n",
    "        subword_to_word_indices_batch = subword_to_word_indices_batch.cuda()\n",
    "        label_batch = label_batch.cuda()\n",
    "\n",
    "    # Forward model\n",
    "    outputs = model(subword_batch, subword_to_word_indices_batch, attention_mask=mask_batch, token_type_ids=token_type_batch, labels=label_batch)\n",
    "    loss, logits = outputs[:2]\n",
    "    \n",
    "    # generate prediction & label list\n",
    "    list_hyps = []\n",
    "    list_labels = []\n",
    "    hyps_list = torch.topk(logits, k=1, dim=-1)[1].squeeze(dim=-1)\n",
    "    for i in range(len(hyps_list)):\n",
    "        hyps, labels = hyps_list[i].tolist(), label_batch[i].tolist()        \n",
    "        list_hyp, list_label = [], []\n",
    "        for j in range(len(hyps)):\n",
    "            if labels[j] == -100:\n",
    "                break\n",
    "            else:\n",
    "                list_hyp.append(i2w[hyps[j]])\n",
    "                list_label.append(i2w[labels[j]])\n",
    "        list_hyps.append(list_hyp)\n",
    "        list_labels.append(list_label)\n",
    "        \n",
    "    return loss, list_hyps, list_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "british-hollow",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\n167574\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "medical-bracket",
   "metadata": {},
   "source": [
    "## Initilaise model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "flying-mistress",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "33"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set random seed\n",
    "set_seed(33)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "growing-cocktail",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NerGritDataset.NUM_LABELS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "italic-bangkok",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForWordClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Load Tokenizer and Config\n",
    "tokenizer = BertTokenizer.from_pretrained('indobenchmark/indobert-base-p1')\n",
    "config = BertConfig.from_pretrained('indobenchmark/indobert-base-p1')\n",
    "config.num_labels = NerGritDataset.NUM_LABELS\n",
    "\n",
    "# Instantiate model\n",
    "model = BertForWordClassification.from_pretrained('indobenchmark/indobert-base-p1', config=config)\n",
    "model= model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "statutory-object",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'I-POI': 0, 'B-POI': 1, 'I-STREET': 2, 'B-STREET': 3, 'O': 4}\n",
      "{0: 'I-POI', 1: 'B-POI', 2: 'I-STREET', 3: 'B-STREET', 4: 'O'}\n"
     ]
    }
   ],
   "source": [
    "w2i, i2w = NerGritDataset.LABEL2INDEX, NerGritDataset.INDEX2LABEL\n",
    "print(w2i)\n",
    "print(i2w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "electric-navigator",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "major-windsor",
   "metadata": {},
   "source": [
    "## Test on sample data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "cutting-ambassador",
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_subword_tokenize(sentence, tokenizer):\n",
    "    # Add CLS token\n",
    "    subwords = [tokenizer.cls_token_id]\n",
    "    subword_to_word_indices = [-1] # For CLS\n",
    "\n",
    "    # Add subwords\n",
    "    for word_idx, word in enumerate(sentence):\n",
    "        subword_list = tokenizer.encode(word, add_special_tokens=False)\n",
    "        subword_to_word_indices += [word_idx for i in range(len(subword_list))]\n",
    "        subwords += subword_list\n",
    "\n",
    "    # Add last SEP token\n",
    "    subwords += [tokenizer.sep_token_id]\n",
    "    subword_to_word_indices += [-1]\n",
    "\n",
    "    return subwords, subword_to_word_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "vietnamese-sauce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>words</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>setu</td>\n",
       "      <td>I-POI</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>siung</td>\n",
       "      <td>I-POI</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>119</td>\n",
       "      <td>I-POI</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>rt</td>\n",
       "      <td>I-POI</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>I-POI</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1</td>\n",
       "      <td>I-POI</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>13880</td>\n",
       "      <td>I-POI</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>cipayung</td>\n",
       "      <td>I-POI</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      words  label\n",
       "0      setu  I-POI\n",
       "1     siung  I-POI\n",
       "2       119  I-POI\n",
       "3        rt  I-POI\n",
       "4         5  I-POI\n",
       "5         1  I-POI\n",
       "6     13880  I-POI\n",
       "7  cipayung  I-POI"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = word_tokenize('setu siung 119 rt 5 1 13880 cipayung')\n",
    "subwords, subword_to_word_indices = word_subword_tokenize(text, tokenizer)\n",
    "\n",
    "subwords = torch.LongTensor(subwords).view(1, -1).to(model.device)\n",
    "subword_to_word_indices = torch.LongTensor(subword_to_word_indices).view(1, -1).to(model.device)\n",
    "logits = model(subwords, subword_to_word_indices)[0]\n",
    "\n",
    "preds = torch.topk(logits, k=1, dim=-1)[1].cpu().squeeze().numpy()\n",
    "labels = [i2w[preds[i]] for i in range(len(preds))]\n",
    "\n",
    "pd.DataFrame({'words': text, 'label': labels})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "color-clear",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "activated-scroll",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## datapath \n",
    "# data = \"F:\\\\Bala_EU_DSVM_BACKUP\\\\Shopee_challenge\\\\Shopee_Challenge\\\\NER_Module\\\\test\\\\data\"\n",
    "# train_file = 'train_preprocess.txt'\n",
    "# validation_file ='valid_preprocess.txt'\n",
    "# test_file = 'test_preprocess_masked_label.txt'\n",
    "# train_data_path = os.path.join(data,train_file)\n",
    "# validation_data_path = os.path.join(data,validation_file)\n",
    "# test_data_path = os.path.join(data,test_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "characteristic-increase",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data = \"F:\\\\Bala_EU_DSVM_BACKUP\\\\Shopee_challenge\\\\Shopee_Challenge\\\\NER_Module\\\\test\\\\data\"\n",
    "# train_file = 'shopee_train.txt'\n",
    "# validation_file ='shopee_test.txt'\n",
    "# test_file = 'test_file.txt'\n",
    "# model_path = 'F:\\\\Bala_EU_DSVM_BACKUP\\\\Shopee_challenge\\\\Shopee_Challenge\\\\model'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "effective-windows",
   "metadata": {},
   "outputs": [],
   "source": [
    "#shoppee dataset\n",
    "model_path = 'F:\\\\Bala_EU_DSVM_BACKUP\\\\Shopee_challenge\\\\Shopee_Challenge\\\\model'\n",
    "data = \"F:\\\\Bala_EU_DSVM_BACKUP\\\\Shopee_challenge\\\\Shopee_Challenge\\\\data_2\"\n",
    "test_data = \"F:\\\\Bala_EU_DSVM_BACKUP\\\\Shopee_challenge\\\\Shopee_Challenge\\\\NER_Module\\\\test\\\\data\"\n",
    "train_file = 'train_80_file_2.txt'\n",
    "validation_file ='val_20_file_2.txt'\n",
    "test_file = 'test_file_2.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "virgin-student",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_path = os.path.join(data,train_file)\n",
    "validation_data_path = os.path.join(data,validation_file)\n",
    "test_data_path = os.path.join(data,test_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "discrete-soldier",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dir = 'F:\\\\Bala_EU_DSVM_BACKUP\\\\Shopee_challenge\\\\Shopee_Challenge\\\\model'\n",
    "if not os.path.exists(model_dir):\n",
    "    os.makedirs(model_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "found-savings",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "train_dataset = NerGritDataset(train_data_path, tokenizer, lowercase=True)\n",
    "valid_dataset = NerGritDataset(validation_data_path, tokenizer, lowercase=True)\n",
    "test_dataset = NerGritDataset(test_data_path, tokenizer, lowercase=True)\n",
    "\n",
    "train_loader = NerDataLoader(dataset=train_dataset, max_seq_len=512, batch_size=16, num_workers=1, shuffle=True)  \n",
    "valid_loader = NerDataLoader(dataset=valid_dataset, max_seq_len=512, batch_size=16, num_workers=1, shuffle=False)  \n",
    "test_loader = NerDataLoader(dataset=test_dataset, max_seq_len=512, batch_size=16, num_workers=1, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "commercial-gazette",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<NER_Module.data.dataset.NerGritDataset at 0x1c14999f250>"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "statistical-placement",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "240000\n",
      "60000\n",
      "50000\n"
     ]
    }
   ],
   "source": [
    "print(len(train_dataset))\n",
    "print(len(valid_dataset))\n",
    "print(len(test_dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "optical-roots",
   "metadata": {},
   "source": [
    "## Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "alternative-cruise",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_val_metric = -100\n",
    "count_stop = 0\n",
    "exp_id = 'notebook_model_18_03_2021'\n",
    "evaluate_every=5\n",
    "step_size=1\n",
    "gamma=0.5\n",
    "early_stop = 3\n",
    "valid_criterion= 'F1'\n",
    "epochs=15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "demographic-large",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(model.parameters(), lr=2e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "going-mixture",
   "metadata": {},
   "outputs": [],
   "source": [
    "## validation evaluate\n",
    "# Evaluate function for validation and test\n",
    "def evaluate(model, data_loader, i2w, is_test=False, device='cpu'):\n",
    "    model.eval()\n",
    "    torch.set_grad_enabled(False)\n",
    "    total_loss, total_correct, total_labels = 0, 0, 0\n",
    "\n",
    "    list_hyp, list_label, list_seq = [], [], []\n",
    "\n",
    "    pbar = tqdm(iter(data_loader), leave=True, total=len(data_loader))\n",
    "    for i, batch_data in enumerate(pbar):\n",
    "        batch_seq = batch_data[-1]        \n",
    "        loss, batch_hyp, batch_label = forward_word_classification(model, batch_data[:-1], i2w=i2w, device= device)\n",
    "\n",
    "        \n",
    "        # Calculate total loss\n",
    "        test_loss = loss.item()\n",
    "        total_loss = total_loss + test_loss\n",
    "\n",
    "        # Calculate evaluation metrics\n",
    "        list_hyp += batch_hyp\n",
    "        list_label += batch_label\n",
    "        list_seq += batch_seq\n",
    "        metrics = ner_metrics_fn(list_hyp, list_label)\n",
    "\n",
    "        if not is_test:\n",
    "            pbar.set_description(\"VALID LOSS:{:.4f} {}\".format(total_loss/(i+1), metrics_to_string(metrics)))\n",
    "        else:\n",
    "            pbar.set_description(\"TEST LOSS:{:.4f} {}\".format(total_loss/(i+1), metrics_to_string(metrics)))\n",
    "    \n",
    "    if is_test:\n",
    "        return total_loss, metrics, list_hyp, list_label, list_seq\n",
    "    else:\n",
    "        return total_loss, metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "unauthorized-omaha",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                                                        | 0/15000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best val is :-100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(Epoch 1) TRAIN LOSS:0.4245 LR:0.00002000: 100%|█████████████████████████████████| 15000/15000 [20:14<00:00, 12.35it/s]\n",
      "  0%|                                                                                        | 0/15000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 1) TRAIN LOSS:0.4245 ACC:0.89 F1:0.68 REC:0.73 PRE:0.63 LR:0.00002000\n",
      "The best val is :-100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(Epoch 2) TRAIN LOSS:nan LR:0.00002000:  68%|████████████████████████▌           | 10209/15000 [13:48<06:28, 12.32it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-28-089cb5ee3ba0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     17\u001b[0m         \u001b[1;31m# Update model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 19\u001b[1;33m         \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     20\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mF:\\anaconda\\envs\\shopee\\lib\\site-packages\\torch\\tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[0;32m    219\u001b[0m                 \u001b[0mretain_graph\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    220\u001b[0m                 create_graph=create_graph)\n\u001b[1;32m--> 221\u001b[1;33m         \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    222\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    223\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mF:\\anaconda\\envs\\shopee\\lib\\site-packages\\torch\\autograd\\__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[0;32m    128\u001b[0m         \u001b[0mretain_graph\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    129\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 130\u001b[1;33m     Variable._execution_engine.run_backward(\n\u001b[0m\u001b[0;32m    131\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    132\u001b[0m         allow_unreachable=True)  # allow_unreachable flag\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Train\n",
    "scheduler = StepLR(optimizer, step_size=step_size, gamma=gamma)\n",
    "n_epochs = epochs\n",
    "for epoch in range(n_epochs):\n",
    "    print('The best val is :{}'.format(best_val_metric))\n",
    "    model.train()\n",
    "    torch.set_grad_enabled(True)\n",
    " \n",
    "    total_train_loss = 0\n",
    "    list_hyp, list_label = [], []\n",
    "\n",
    "    train_pbar = tqdm(train_loader, leave=True, total=len(train_loader))\n",
    "    for i, batch_data in enumerate(train_pbar):\n",
    "        # Forward model\n",
    "        loss, batch_hyp, batch_label = forward_word_classification(model, batch_data[:-1], i2w=i2w, device='cuda')\n",
    "\n",
    "        # Update model\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        tr_loss = loss.item()\n",
    "        total_train_loss = total_train_loss + tr_loss\n",
    "\n",
    "        # Calculate metrics\n",
    "        list_hyp += batch_hyp\n",
    "        list_label += batch_label\n",
    "\n",
    "        train_pbar.set_description(\"(Epoch {}) TRAIN LOSS:{:.4f} LR:{:.8f}\".format((epoch+1),\n",
    "            total_train_loss/(i+1), get_lr(optimizer)))\n",
    "\n",
    "    # Calculate train metric\n",
    "    metrics = ner_metrics_fn(list_hyp, list_label)\n",
    "    print(\"(Epoch {}) TRAIN LOSS:{:.4f} {} LR:{:.8f}\".format((epoch+1),\n",
    "        total_train_loss/(i+1), metrics_to_string(metrics), get_lr(optimizer)))\n",
    "\n",
    "    # Evaluate on validation\n",
    "    # evaluate\n",
    "    if ((epoch+1) % evaluate_every) == 0:\n",
    "        val_loss, val_metrics = evaluate(model, valid_loader, i2w, is_test=False, device ='cuda')\n",
    "        print('(Epoch {}) VAL_METRIC :{:.4f}'.format((epoch+1),val_metrics[valid_criterion]))\n",
    "        # Early stopping\n",
    "        val_metric = val_metrics[valid_criterion]\n",
    "        if best_val_metric < val_metric:\n",
    "            best_val_metric = val_metric\n",
    "            # save model\n",
    "            if exp_id is not None:\n",
    "                torch.save(model.state_dict(), model_dir + \"/best_model_\" + str(exp_id) + \".th\")\n",
    "            else:\n",
    "                torch.save(model.state_dict(), model_dir + \"/best_model.th\")\n",
    "            count_stop = 0\n",
    "        else:\n",
    "            print('The best val is :{} and val_metric is {}'.format(best_val_metric,val_metric))\n",
    "            count_stop += 1\n",
    "            print(\"count stop: {}\".format(count_stop))\n",
    "            if count_stop == early_stop:\n",
    "                break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "north-permit",
   "metadata": {},
   "source": [
    "## Predict on test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adverse-recorder",
   "metadata": {},
   "outputs": [],
   "source": [
    "## load model if not there\n",
    "\n",
    "# define model path \n",
    "# model_folder = './'\n",
    "# filename = 'best_model_1.th'\n",
    "# model_path = os.path.join(model_folder,filename)\n",
    "\n",
    "# def load_model(model_path):\n",
    "#     model.load_state_dict(torch.load(model_path))\n",
    "#     model = model.cuda()\n",
    "#     return model\n",
    "\n",
    "# trained_model = load_model(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dress-bracket",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate on test\n",
    "model.eval()\n",
    "torch.set_grad_enabled(False)\n",
    "\n",
    "total_loss, total_correct, total_labels = 0, 0, 0\n",
    "list_hyp, list_label = [], []\n",
    "\n",
    "pbar = tqdm(test_loader, leave=True, total=len(test_loader))\n",
    "for i, batch_data in enumerate(pbar):\n",
    "    _, batch_hyp, _ = forward_word_classification(model, batch_data[:-1], i2w=i2w, device='cuda')\n",
    "    list_hyp += batch_hyp\n",
    "\n",
    "# Save prediction\n",
    "result_df = pd.DataFrame({'label':list_hyp}).reset_index()\n",
    "print(result_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "continuous-potato",
   "metadata": {},
   "source": [
    "### Save result in csv "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acceptable-substitute",
   "metadata": {},
   "outputs": [],
   "source": [
    "## path \n",
    "save_path = model_path\n",
    "filename = 'shopee_notebook'\n",
    "filename = filename + '.csv'\n",
    "result_path = os.path.join(save_path, filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "comparable-title",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df.to_csv(result_path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sufficient-dressing",
   "metadata": {},
   "source": [
    "## Test on sample sentecnces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "earlier-encounter",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = word_tokenize('Jalan Candi Panggung Barat. No 16 . RT 01 RW 18. Kelurahan Mojolangu, Kecamatan Lowokwaru Malang City , East Java')\n",
    "subwords, subword_to_word_indices = word_subword_tokenize(text, tokenizer)\n",
    "\n",
    "subwords = torch.LongTensor(subwords).view(1, -1).to(model.device)\n",
    "subword_to_word_indices = torch.LongTensor(subword_to_word_indices).view(1, -1).to(model.device)\n",
    "logits = model(subwords, subword_to_word_indices)[0]\n",
    "\n",
    "preds = torch.topk(logits, k=1, dim=-1)[1].squeeze().cpu().numpy()\n",
    "labels = [i2w[preds[i]] for i in range(len(preds))]\n",
    "\n",
    "pd.DataFrame({'words': text, 'label': labels})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "public-chuck",
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_model = load_model(model_path=model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "initial-disney",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from NER_Module import execute_main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "amber-advice",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = 'F:\\\\Bala_EU_DSVM_BACKUP\\\\Shopee_challenge\\\\Shopee_Challenge\\\\model'\n",
    "data = \"F:\\\\Bala_EU_DSVM_BACKUP\\\\Shopee_challenge\\\\Shopee_Challenge\\\\data_3\"\n",
    "test_data = \"F:\\\\Bala_EU_DSVM_BACKUP\\\\Shopee_challenge\\\\Shopee_Challenge\\\\NER_Module\\\\test\\\\data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "fabulous-valley",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_file = 'test_file.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "greatest-float",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_path = os.path.join(data,test_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "operational-travel",
   "metadata": {},
   "outputs": [],
   "source": [
    "## model_path \n",
    "model_folder = './'\n",
    "filename = 'best_model_pipeline_final_model.th'\n",
    "trained_model_path = os.path.join(model_folder,filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "sufficient-casino",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_path ='F:\\\\Bala_EU_DSVM_BACKUP\\\\Shopee_challenge\\\\Shopee_Challenge\\\\model'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "saved-antique",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-03-20 09:32:32,842 [NER_Module.model.model] [INFO] Loading pretrained indobert model.....\n",
      "INFO:NER_Module.model.model:Loading pretrained indobert model.....\n",
      "Some weights of BertForWordClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "2021-03-20 09:32:41,842 [NER_Module.model.model] [INFO] Initialised model with indo-bert!!!!\n",
      "INFO:NER_Module.model.model:Initialised model with indo-bert!!!!\n",
      "2021-03-20 09:32:41,843 [NER_Module.model.model] [INFO] Device: cuda\n",
      "INFO:NER_Module.model.model:Device: cuda\n",
      "2021-03-20 09:32:41,845 [NER_Module.model.model] [INFO] Loading model from local path in cuda device\n",
      "INFO:NER_Module.model.model:Loading model from local path in cuda device\n",
      "2021-03-20 09:32:42,489 [NER_Module.model.model] [INFO] Loading Tokenizer from indo-Bert\n",
      "INFO:NER_Module.model.model:Loading Tokenizer from indo-Bert\n",
      "2021-03-20 09:32:43,189 [NER_Module.model.model] [INFO] Prediction mode....\n",
      "INFO:NER_Module.model.model:Prediction mode....\n",
      "2021-03-20 09:32:43,191 [NER_Module.model.model] [INFO] Total sentence to predict: 3125\n",
      "INFO:NER_Module.model.model:Total sentence to predict: 3125\n",
      "100%|██████████████████████████████████████████████████████████████████████████████| 3125/3125 [04:13<00:00, 12.32it/s]\n",
      "2021-03-20 09:36:57,031 [NER_Module.model.model] [INFO] Results saved in F:\\Bala_EU_DSVM_BACKUP\\Shopee_challenge\\Shopee_Challenge\\model\\shopee_pipeline_final.csv\n",
      "INFO:NER_Module.model.model:Results saved in F:\\Bala_EU_DSVM_BACKUP\\Shopee_challenge\\Shopee_Challenge\\model\\shopee_pipeline_final.csv\n",
      "2021-03-20 09:36:57,032 [NER_Module.model.model] [INFO] Prediction completed!!!\n",
      "INFO:NER_Module.model.model:Prediction completed!!!\n",
      "2021-03-20 09:36:57,033 [NER_Module.model.model] [INFO] Total prediction time is:253.8401656150818\n",
      "INFO:NER_Module.model.model:Total prediction time is:253.8401656150818\n",
      "2021-03-20 09:36:57,035 [NER_Module.executor] [INFO] Results are not converted into shopee format\n",
      "INFO:NER_Module.executor:Results are not converted into shopee format\n"
     ]
    }
   ],
   "source": [
    "## model initialisation\n",
    "predict_df = execute_main(train_dataset_path=None, valid_dataset_path=None,\n",
    "                          test_dataset_path=test_data_path, model_dir=model_folder, model_filename=filename,\n",
    "                          predict_only=True, random_state=33, device='cuda',\n",
    "                          result_path=result_path, result_filename='shopee_pipeline_final',\n",
    "                          convert_shopee=False, shopee_prediction_file=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "silent-vision",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "healthy-render",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "radical-elephant",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python(shopee)",
   "language": "python",
   "name": "shopee"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
